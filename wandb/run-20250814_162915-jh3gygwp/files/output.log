[34m[1mwandb[0m: [33mWARNING[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).
  0%|                                                                                                     | 0/1500 [00:00<?, ?it/s]D:\MLLMs\MLLM\.venv\Lib\site-packages\torch\nn\modules\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\aten\src\ATen\native\cudnn\Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                            | 500/1500 [01:49<03:49,  4.36it/s]CorenetToHFPretrainedModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
{'loss': 6.3518, 'grad_norm': 34.84500503540039, 'learning_rate': 2.2583037468791826e-05, 'epoch': 0.13}
{'loss': 2.1562, 'grad_norm': 41.13071060180664, 'learning_rate': 4.562695325327328e-05, 'epoch': 0.27}
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
{'eval_loss': 1.669845461845398, 'eval_accuracy': 0.41, 'eval_runtime': 0.5104, 'eval_samples_per_second': 195.915, 'eval_steps_per_second': 13.714, 'epoch': 0.27}
{'loss': 1.6359, 'grad_norm': 25.211559295654297, 'learning_rate': 5.414318299971208e-05, 'epoch': 0.4}
{'loss': 1.3454, 'grad_norm': 39.09198760986328, 'learning_rate': 5.2139364235844126e-05, 'epoch': 0.53}
{'eval_loss': 1.183419942855835, 'eval_accuracy': 0.54, 'eval_runtime': 0.4919, 'eval_samples_per_second': 203.296, 'eval_steps_per_second': 14.231, 'epoch': 0.53}
{'loss': 1.2777, 'grad_norm': 29.75149917602539, 'learning_rate': 5.0135545471976175e-05, 'epoch': 0.67}
{'loss': 1.0906, 'grad_norm': 26.150978088378906, 'learning_rate': 4.8131726708108224e-05, 'epoch': 0.8}
{'eval_loss': 0.9918360710144043, 'eval_accuracy': 0.64, 'eval_runtime': 0.4975, 'eval_samples_per_second': 201.007, 'eval_steps_per_second': 14.07, 'epoch': 0.8}
{'loss': 0.996, 'grad_norm': 37.666748046875, 'learning_rate': 4.6127907944240266e-05, 'epoch': 0.93}
{'loss': 0.8283, 'grad_norm': 23.842172622680664, 'learning_rate': 4.4124089180372315e-05, 'epoch': 1.07}
{'eval_loss': 0.942721426486969, 'eval_accuracy': 0.64, 'eval_runtime': 0.4649, 'eval_samples_per_second': 215.11, 'eval_steps_per_second': 15.058, 'epoch': 1.07}
{'loss': 0.6508, 'grad_norm': 20.17266845703125, 'learning_rate': 4.2120270416504364e-05, 'epoch': 1.2}
{'loss': 0.6219, 'grad_norm': 36.977108001708984, 'learning_rate': 4.0116451652636406e-05, 'epoch': 1.33}
{'eval_loss': 1.0047717094421387, 'eval_accuracy': 0.67, 'eval_runtime': 0.7119, 'eval_samples_per_second': 140.478, 'eval_steps_per_second': 9.833, 'epoch': 1.33}
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
CorenetToHFPretrainedModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                            | 504/1500 [01:50<06:41,  2.48it/s]D:\MLLMs\MLLM\.venv\Lib\site-packages\torch\nn\modules\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\aten\src\ATen\native\cudnn\Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 1000/1500 [03:39<01:35,  5.25it/s]CorenetToHFPretrainedModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
{'loss': 0.5674, 'grad_norm': 43.77031707763672, 'learning_rate': 3.8112632888768455e-05, 'epoch': 1.47}
{'loss': 0.5929, 'grad_norm': 23.46299171447754, 'learning_rate': 3.6108814124900504e-05, 'epoch': 1.6}
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
{'eval_loss': 0.7757748961448669, 'eval_accuracy': 0.74, 'eval_runtime': 0.5021, 'eval_samples_per_second': 199.182, 'eval_steps_per_second': 13.943, 'epoch': 1.6}
{'loss': 0.566, 'grad_norm': 22.76167869567871, 'learning_rate': 3.410499536103255e-05, 'epoch': 1.73}
{'loss': 0.5477, 'grad_norm': 35.70747375488281, 'learning_rate': 3.2101176597164595e-05, 'epoch': 1.87}
{'eval_loss': 0.807951807975769, 'eval_accuracy': 0.73, 'eval_runtime': 1.1039, 'eval_samples_per_second': 90.588, 'eval_steps_per_second': 6.341, 'epoch': 1.87}
{'loss': 0.4774, 'grad_norm': 26.006694793701172, 'learning_rate': 3.009735783329665e-05, 'epoch': 2.0}
{'loss': 0.1345, 'grad_norm': 35.885746002197266, 'learning_rate': 2.8093539069428693e-05, 'epoch': 2.13}
{'eval_loss': 0.9549038410186768, 'eval_accuracy': 0.73, 'eval_runtime': 0.8364, 'eval_samples_per_second': 119.566, 'eval_steps_per_second': 8.37, 'epoch': 2.13}
{'loss': 0.1402, 'grad_norm': 9.53714656829834, 'learning_rate': 2.6089720305560742e-05, 'epoch': 2.27}
{'loss': 0.1789, 'grad_norm': 14.676220893859863, 'learning_rate': 2.408590154169279e-05, 'epoch': 2.4}
{'eval_loss': 0.9124361276626587, 'eval_accuracy': 0.7, 'eval_runtime': 0.5241, 'eval_samples_per_second': 190.799, 'eval_steps_per_second': 13.356, 'epoch': 2.4}
{'loss': 0.1641, 'grad_norm': 48.06108093261719, 'learning_rate': 2.2082082777824837e-05, 'epoch': 2.53}
{'loss': 0.1517, 'grad_norm': 13.483036994934082, 'learning_rate': 2.0078264013956886e-05, 'epoch': 2.67}
{'eval_loss': 0.8710629343986511, 'eval_accuracy': 0.77, 'eval_runtime': 0.4926, 'eval_samples_per_second': 203.013, 'eval_steps_per_second': 14.211, 'epoch': 2.67}
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
CorenetToHFPretrainedModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
D:\MLLMs\MLLM\.venv\Lib\site-packages\torch\nn\modules\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\aten\src\ATen\native\cudnn\Conv_v8.cpp:919.)
  return F.conv1d(input, weight, bias, self.stride,
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [05:20<00:00,  5.24it/s]CorenetToHFPretrainedModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
{'loss': 0.1178, 'grad_norm': 0.9776356816291809, 'learning_rate': 1.807444525008893e-05, 'epoch': 2.8}
{'loss': 0.1138, 'grad_norm': 40.483524322509766, 'learning_rate': 1.607062648622098e-05, 'epoch': 2.93}
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
{'eval_loss': 0.9392937421798706, 'eval_accuracy': 0.76, 'eval_runtime': 0.4735, 'eval_samples_per_second': 211.18, 'eval_steps_per_second': 14.783, 'epoch': 2.93}
{'loss': 0.0752, 'grad_norm': 9.306288719177246, 'learning_rate': 1.4066807722353026e-05, 'epoch': 3.07}
{'loss': 0.0147, 'grad_norm': 0.8305465579032898, 'learning_rate': 1.2062988958485075e-05, 'epoch': 3.2}
{'eval_loss': 0.990793764591217, 'eval_accuracy': 0.75, 'eval_runtime': 0.5006, 'eval_samples_per_second': 199.762, 'eval_steps_per_second': 13.983, 'epoch': 3.2}
{'loss': 0.0157, 'grad_norm': 1.402596354484558, 'learning_rate': 1.005917019461712e-05, 'epoch': 3.33}
{'loss': 0.0132, 'grad_norm': 2.4644055366516113, 'learning_rate': 8.05535143074917e-06, 'epoch': 3.47}
{'eval_loss': 1.1262214183807373, 'eval_accuracy': 0.73, 'eval_runtime': 0.4874, 'eval_samples_per_second': 205.184, 'eval_steps_per_second': 14.363, 'epoch': 3.47}
{'loss': 0.0158, 'grad_norm': 2.1323556900024414, 'learning_rate': 6.051532666881217e-06, 'epoch': 3.6}
{'loss': 0.0147, 'grad_norm': 0.5904030799865723, 'learning_rate': 4.047713903013264e-06, 'epoch': 3.73}
{'eval_loss': 1.1108955144882202, 'eval_accuracy': 0.75, 'eval_runtime': 0.5196, 'eval_samples_per_second': 192.47, 'eval_steps_per_second': 13.473, 'epoch': 3.73}
{'loss': 0.0078, 'grad_norm': 1.2264467477798462, 'learning_rate': 2.0438951391453118e-06, 'epoch': 3.87}
{'loss': 0.0122, 'grad_norm': 11.450474739074707, 'learning_rate': 4.007637527735905e-08, 'epoch': 4.0}
{'eval_loss': 1.068131446838379, 'eval_accuracy': 0.75, 'eval_runtime': 0.5237, 'eval_samples_per_second': 190.935, 'eval_steps_per_second': 13.365, 'epoch': 4.0}
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
CorenetToHFPretrainedModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [05:20<00:00,  4.68it/s]
{'train_runtime': 334.1015, 'train_samples_per_second': 71.834, 'train_steps_per_second': 4.49, 'train_loss': 0.6958742975393931, 'epoch': 4.0}
