开始ByteFormer + GPT2 Caption训练...
训练样本数: 30000
验证样本数: 50
训练参数: MySeq2SeqTrainingArguments(output_dir='./checkpoints/byteformer_gpt2_caption', train_batch_size=8, eval_batch_size=16, eval_strategy='steps', eval_steps=600, logging_steps=50, save_steps=600, warmup_steps=187, learning_rate=5e-05, num_train_epochs=5, save_total_limit=3, lr_scheduler_type='cosine', gradient_accumulation_steps=2, fp16=True, bf16=False, report_to='wandb', wandb_project='my_project', wandb_run_name=None, tb_log_dir='./runs')
模型结构:
CustomEncoderDecoderModel(
  (encoder): ByteFormerWrapper(
    (byteformer): ByteFormer(
      (embeddings): Embedding(257, 192, padding_idx=256)
      (token_reduction_net): Conv1d(192, 192, kernel_size=(4,), stride=(2,), bias=False)
      (pos_embed): LearnablePositionalEmbedding(num_embeddings=50000, embedding_dim=192, padding_idx=None, sequence_first=False)
      (emb_dropout): Dropout(p=0.0, inplace=False)
      (downsamplers): ModuleDict(
        (downsample_0): TokenMerging(
          dim=192, window=2
          (reduction): LinearLayer(in_features=384, out_features=192, bias=False, channel_first=False)
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        )
        (downsample_1): TokenMerging(
          dim=192, window=2
          (reduction): LinearLayer(in_features=384, out_features=192, bias=False, channel_first=False)
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        )
        (downsample_3): TokenMerging(
          dim=192, window=2
          (reduction): LinearLayer(in_features=384, out_features=192, bias=False, channel_first=False)
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        )
        (downsample_5): TokenMerging(
          dim=192, window=2
          (reduction): LinearLayer(in_features=384, out_features=192, bias=False, channel_first=False)
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        )
        (downsample_7): TokenMerging(
          dim=192, window=2
          (reduction): LinearLayer(in_features=384, out_features=192, bias=False, channel_first=False)
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        )
        (downsample_9): TokenMerging(
          dim=192, window=2
          (reduction): LinearLayer(in_features=384, out_features=192, bias=False, channel_first=False)
          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer): Sequential(
        (0): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 0)
        (1): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 64)
        (2): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 0)
        (3): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 64)
        (4): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 0)
        (5): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 64)
        (6): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 0)
        (7): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 64)
        (8): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 0)
        (9): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 64)
        (10): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 0)
        (11): WindowedTransformerEncoder(embed_dim=192, ffn_dim=768, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm, 128, 64)
      )
      (post_transformer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): CustomGPT2LMHeadModel(
    (transformer): GPT2Model(
      (wte): Embedding(50257, 768)
      (wpe): Embedding(1024, 768)
      (drop): Dropout(p=0.1, inplace=False)
      (h): ModuleList(
        (0-11): 12 x GPT2Block(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            (c_attn): Conv1D(nf=2304, nx=768)
            (c_proj): Conv1D(nf=768, nx=768)
            (attn_dropout): Dropout(p=0.1, inplace=False)
            (resid_dropout): Dropout(p=0.1, inplace=False)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (crossattention): GPT2Attention(
            (c_attn): Conv1D(nf=1536, nx=768)
            (q_attn): Conv1D(nf=768, nx=768)
            (c_proj): Conv1D(nf=768, nx=768)
            (attn_dropout): Dropout(p=0.1, inplace=False)
            (resid_dropout): Dropout(p=0.1, inplace=False)
          )
          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            (c_fc): Conv1D(nf=3072, nx=768)
            (c_proj): Conv1D(nf=768, nx=3072)
            (act): NewGELUActivation()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (lm_head): Linear(in_features=768, out_features=50257, bias=False)
  )
  (enc_to_dec_proj): Linear(in_features=192, out_features=768, bias=True)
)
Training:   0%|                                                           | 0/18750 [00:00<?, ?it/s]/root/autodl-tmp/corenet/byteformer-hf-migration/scripts/train_byteformer_gpt2_caption.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).

==== 训练前检查 ====
模型设备: cuda:0
Trainer设备: cuda
模型类型: <class '__main__.CustomEncoderDecoderModel'>
可训练参数: 413 / 413
==== 检查完成 ====

[DEBUG] 第一个batch设备检查:
  input_ids设备: cuda:0
  labels设备: cuda:0
  模型设备: cuda:0
  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)
Training:   0%|    | 81/18750 [00:10<38:32,  8.07it/s, ep=0.02/5, step=81, loss=2.1153, lr=5.00e-05]Traceback (most recent call last):
[Batch    50] [Opt   25] [Ep  0.013] | Loss:  2.6014 | GradNorm: 235215.312 | LR: 5.00e-05
  File "/root/autodl-tmp/corenet/byteformer-hf-migration/scripts/train_byteformer_gpt2_caption.py", line 621, in <module>
    main()
  File "/root/autodl-tmp/corenet/byteformer-hf-migration/scripts/train_byteformer_gpt2_caption.py", line 586, in main
    trainer.train()
  File "/root/autodl-tmp/corenet/byteformer-hf-migration/utils/hf_style_trainer.py", line 260, in train
    grad_norm = self._compute_grad_norm()
                ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/corenet/byteformer-hf-migration/utils/hf_style_trainer.py", line 423, in _compute_grad_norm
    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2.0).to(device) for p in parameters]), 2.0)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
